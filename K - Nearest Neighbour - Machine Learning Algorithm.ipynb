{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90e0e92f",
   "metadata": {},
   "source": [
    "## K Nearest Neighbour\n",
    "\n",
    "K Nearest Neighbors (KNN) is a popular and simple machine learning algorithm used for classification and regression tasks. It is a non-parametric and instance-based learning algorithm, meaning it doesn't make any assumptions about the underlying data distribution and stores the entire training dataset to make predictions. Here's how the KNN algorithm works:\n",
    "\n",
    "1. Data Preparation: Gather a labeled dataset with features and corresponding target labels. KNN can be used for both classification (discrete labels) and regression (continuous labels) tasks.\n",
    "\n",
    "2. Choosing 'K': Select a value for 'K', which represents the number of nearest neighbors to consider when making predictions. It is a hyperparameter, and the choice of 'K' can significantly impact the algorithm's performance.\n",
    "\n",
    "3. Distance Metric: Determine a distance metric to measure the similarity between data points. Common distance metrics include Euclidean distance, Manhattan distance, Minkowski distance, etc.\n",
    "\n",
    "4. Prediction: When a new data point (query point) needs to be classified or predicted, the algorithm finds the 'K' closest data points (nearest neighbors) based on the chosen distance metric.\n",
    "\n",
    "5. Voting (Classification) or Averaging (Regression): For classification tasks, the algorithm takes a majority vote among the 'K' nearest neighbors and assigns the most common class label to the new data point. For regression tasks, it calculates the average of the target values of the 'K' nearest neighbors and assigns that value as the prediction for the new data point.\n",
    "\n",
    "6. Choosing the Output: In some cases, you may need to decide how to handle ties (e.g., two classes with equal votes in classification). Various approaches can be used, such as considering the distance of neighbors or using weighted voting.\n",
    "\n",
    "7. Model Evaluation: Finally, evaluate the performance of the KNN model using appropriate evaluation metrics such as accuracy, precision, recall, F1 score, mean squared error (MSE), etc.\n",
    "\n",
    "It's important to note that KNN can be sensitive to the choice of 'K' and the distance metric. A small 'K' value can lead to overfitting, while a large 'K' value can cause underfitting. Additionally, feature scaling is recommended, as KNN is distance-based and features on different scales can dominate the distance calculations.\n",
    "\n",
    "While KNN is simple and intuitive, it can be computationally expensive for large datasets since it requires comparing the query point to all data points during prediction. However, it can be an effective choice for smaller datasets or as a baseline model to establish a performance benchmark for more complex algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a6e2871",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1889abde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83e54040",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(\n",
    "    n_samples=1000, # 1000 observations \n",
    "    n_features=3, # 3 total features\n",
    "     n_redundant=1,\n",
    "    n_classes=2, # binary target/label \n",
    "    random_state=999 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e3f3ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccc354f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5595c03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c37ffbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier=KNeighborsClassifier(n_neighbors=5,algorithm='auto')\n",
    "classifier.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e311e000",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    }
   ],
   "source": [
    "y_pred=classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3937e465",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,accuracy_score,classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f737246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[158  20]\n",
      " [ 11 141]]\n",
      "0.906060606060606\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.89      0.91       178\n",
      "           1       0.88      0.93      0.90       152\n",
      "\n",
      "    accuracy                           0.91       330\n",
      "   macro avg       0.91      0.91      0.91       330\n",
      "weighted avg       0.91      0.91      0.91       330\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_pred,y_test))\n",
    "print(accuracy_score(y_pred,y_test))\n",
    "print(classification_report(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e154c1f",
   "metadata": {},
   "source": [
    "## K Nearest Neighbour Regressor\n",
    "\n",
    "K Nearest Neighbors (KNN) is a simple and popular supervised machine learning algorithm used for both classification and regression tasks. Here, we'll focus on the K Nearest Neighbors Regressor, which is specifically used for regression problems. The basic idea behind the KNN Regressor is to predict the value of a data point by averaging the values of its K nearest neighbors. In the context of regression, this means finding the K data points with the closest feature values to the input data point and averaging their target values to make the final prediction. Here's how the KNN Regressor works:\n",
    "\n",
    "1. Data Preparation: Ensure your dataset is properly cleaned and preprocessed, with features and corresponding target values.\n",
    "\n",
    "2. Choosing K: The first step is to choose the value of K, which represents the number of neighbors to consider. A smaller K will make the model more sensitive to noise, while a larger K may smooth out the predictions but could lead to less localized and less accurate results. The best K value is usually determined through cross-validation techniques.\n",
    "\n",
    "3. Distance Metric: Next, you need to define a distance metric to measure the similarity between data points. The most common distance metrics for continuous features are Euclidean distance or Manhattan distance. These distance metrics help find the K closest neighbors to a given data point.\n",
    "\n",
    "4. Prediction: For each data point in the test set, the KNN Regressor algorithm does the following:\n",
    "\n",
    "   - Calculate the distance between the test data point and all the data points in the training set using the chosen distance metric.\n",
    "   - Sort the distances in ascending order and select the K nearest neighbors.\n",
    "   - Calculate the average (or weighted average) of the target values of the K neighbors. This average will be the predicted value for the test data point.\n",
    "\n",
    "5. Regression Evaluation: After making predictions for all test data points, evaluate the performance of the model using regression evaluation metrics like Mean Squared Error (MSE), Mean Absolute Error (MAE), or R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d82b3b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "X, y = make_regression(n_samples=1000, n_features=2, noise=10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8caf1db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc14e74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6480c118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsRegressor(n_neighbors=6)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor=KNeighborsRegressor(n_neighbors=6,algorithm='auto')\n",
    "regressor.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50dc0b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c352a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score,mean_absolute_error,mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c99891ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9189275159979495\n",
      "9.009462452972217\n",
      "127.45860414317289\n"
     ]
    }
   ],
   "source": [
    "print(r2_score(y_test,y_pred))\n",
    "print(mean_absolute_error(y_test,y_pred))\n",
    "print(mean_squared_error(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f5bbea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b019e53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c1fbed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2708965",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fab8999",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
