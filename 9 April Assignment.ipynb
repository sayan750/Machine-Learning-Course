{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c46aaf60",
   "metadata": {},
   "source": [
    "### 1.\n",
    "\n",
    "Bayes' theorem, named after Thomas Bayes, is a fundamental concept in probability theory and statistics. It describes how to update or revise the probability of a hypothesis or event based on new evidence or information.\n",
    "\n",
    "The theorem is expressed mathematically as:\n",
    "\n",
    "P(A|B) = (P(B|A) * P(A)) / P(B)\n",
    "\n",
    "where:\n",
    "\n",
    "- P(A|B) is the probability of event A given that event B has occurred. This is called the posterior probability.\n",
    "- P(B|A) is the probability of event B given that event A is true. This is called the likelihood.\n",
    "- P(A) and P(B) are the probabilities of events A and B occurring independently of each other. These are called the prior probabilities of A and B, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102c6dca",
   "metadata": {},
   "source": [
    "### 2.\n",
    "\n",
    "The formula for Bayes' theorem is as follows:\n",
    "\n",
    "P(A|B) = (P(B|A) * P(A)) / P(B)\n",
    "\n",
    "where:\n",
    "\n",
    "- P(A|B) represents the probability of event A occurring given that event B has occurred.\n",
    "- P(B|A) represents the probability of event B occurring given that event A has occurred.\n",
    "- P(A) represents the prior probability of event A.\n",
    "- P(B) represents the prior probability of event B."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e6382f",
   "metadata": {},
   "source": [
    "### 3.\n",
    "\n",
    "Bayes' theorem is a fundamental concept in probability theory and statistics that is widely used in various practical applications. Here are some common areas where Bayes' theorem is employed:\n",
    "\n",
    "1. Medical diagnosis: Bayes' theorem plays a crucial role in medical diagnostics. By combining prior knowledge (prevalence of a disease in a population) with diagnostic test results (sensitivity and specificity), Bayes' theorem can help calculate the probability of a person having a disease given their test results.\n",
    "\n",
    "2. Spam filtering: Email providers often use Bayes' theorem for spam filtering. By analyzing the probability of certain words or phrases appearing in spam emails versus legitimate emails (based on training data), an email system can classify incoming emails as spam or not spam.\n",
    "\n",
    "3. Document classification: In natural language processing, Bayes' theorem is used for document classification tasks, such as sentiment analysis or topic classification. By considering the probability distribution of words in different categories (e.g., positive or negative sentiment), Bayes' theorem can assign a probability of a document belonging to a particular category.\n",
    "\n",
    "4. Fault diagnosis in engineering: Bayes' theorem is applied to fault diagnosis in engineering systems. By considering prior knowledge about system behavior and the observations of various sensor readings, Bayes' theorem can help infer the most likely cause of a fault or failure in the system.\n",
    "\n",
    "5. Machine learning: Bayesian inference, which relies on Bayes' theorem, is used in various machine learning algorithms. Bayesian methods allow for the estimation of model parameters and can provide uncertainty estimates in predictions, enabling more robust decision-making.\n",
    "\n",
    "6. A/B testing: In marketing and web analytics, A/B testing is used to compare the performance of different versions of a webpage or a marketing campaign. Bayes' theorem can be employed to update the belief in the effectiveness of different versions based on observed user interactions and conversions.\n",
    "\n",
    "7. Prediction and forecasting: Bayes' theorem is utilized in predictive modeling and forecasting tasks. By incorporating prior beliefs and updating them based on new evidence, Bayes' theorem enables the estimation of future events or the prediction of outcomes based on available data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905e8f35",
   "metadata": {},
   "source": [
    "### 4.\n",
    "\n",
    "Bayes' theorem is a fundamental result in probability theory that establishes a relationship between conditional probabilities. It provides a way to update or revise our beliefs or probabilities based on new evidence.\n",
    "\n",
    "Conditional probability is the probability of an event occurring given that another event has already occurred. It is denoted as P(A|B), which represents the probability of event A happening given that event B has already occurred.\n",
    "\n",
    "Bayes' theorem relates conditional probabilities by expressing the probability of event A given event B in terms of the probability of event B given event A, along with the prior probabilities of events A and B."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aea032c",
   "metadata": {},
   "source": [
    "### 5.\n",
    "\n",
    "Here are the three commonly used Naive Bayes classifiers and factors to consider when choosing among them:\n",
    "\n",
    "Gaussian Naive Bayes:\n",
    "\n",
    "- Suitable for continuous features that follow a Gaussian (normal) distribution.\n",
    "- Assumes that the continuous features are independent and have a normal distribution.\n",
    "- If your data contains continuous variables, and the distribution of these variables approximates a Gaussian distribution, Gaussian Naive Bayes can be a good choice.\n",
    "\n",
    "Multinomial Naive Bayes:\n",
    "\n",
    "- Appropriate for discrete features with discrete counts, such as word frequencies in text classification.\n",
    "- Assumes that the features are independent and generated from a multinomial distribution.\n",
    "- Commonly used in text classification problems where the data is represented as word frequencies or document-term matrices.\n",
    "\n",
    "Bernoulli Naive Bayes:\n",
    "\n",
    "- Suitable for binary features, where each feature can take only two values (0 or 1).\n",
    "- Assumes that features are independent and generated from a Bernoulli distribution.\n",
    "- Often used in sentiment analysis or document classification tasks when working with binary feature representations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b94bce",
   "metadata": {},
   "source": [
    "### 6.\n",
    "\n",
    "To predict the class of a new instance using Naive Bayes, we need to calculate the posterior probabilities for each class given the feature values of the new instance. The Naive Bayes assumption assumes independence between features given the class.\n",
    "\n",
    "First, let's calculate the prior probabilities for each class, assuming equal prior probabilities:\n",
    "\n",
    "P(A) = P(B) = 0.5\n",
    "\n",
    "Now, we need to calculate the likelihood probabilities for each feature value given each class. Since the feature values are discrete, we can use the frequency counts from the table.\n",
    "\n",
    "For Class A:\n",
    "\n",
    "P(X1=3|A) = 4/16 = 0.25\n",
    "\n",
    "P(X2=4|A) = 3/16 = 0.1875\n",
    "\n",
    "For Class B:\n",
    "\n",
    "P(X1=3|B) = 1/9 ≈ 0.1111\n",
    "\n",
    "P(X2=4|B) = 3/9 = 0.3333\n",
    "\n",
    "Next, we calculate the evidence or marginal likelihood, which is the probability of observing the feature values regardless of the class:\n",
    "\n",
    "P(X1=3) = P(X1=3|A) * P(A) + P(X1=3|B) * P(B)\n",
    "= 0.25 * 0.5 + 0.1111 * 0.5\n",
    "≈ 0.1806\n",
    "\n",
    "P(X2=4) = P(X2=4|A) * P(A) + P(X2=4|B) * P(B)\n",
    "= 0.1875 * 0.5 + 0.3333 * 0.5\n",
    "≈ 0.2604\n",
    "\n",
    "Now, we can calculate the posterior probabilities using Bayes' theorem:\n",
    "\n",
    "P(A|X1=3, X2=4) = (P(X1=3|A) * P(X2=4|A) * P(A)) / (P(X1=3) * P(X2=4))\n",
    "= (0.25 * 0.1875 * 0.5) / (0.1806 * 0.2604)\n",
    "≈ 0.598\n",
    "\n",
    "P(B|X1=3, X2=4) = (P(X1=3|B) * P(X2=4|B) * P(B)) / (P(X1=3) * P(X2=4))\n",
    "= (0.1111 * 0.3333 * 0.5) / (0.1806 * 0.2604)\n",
    "≈ 0.402\n",
    "\n",
    "Therefore, Naive Bayes predicts that the new instance with X1 = 3 and X2 = 4 belongs to Class A, as it has a higher posterior probability compared to Class B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06a05c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c99be3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b2ab5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26040ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f54e19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
