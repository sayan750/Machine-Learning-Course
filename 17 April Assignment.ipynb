{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cf81597",
   "metadata": {},
   "source": [
    "## 1.\n",
    "\n",
    "Gradient Boosting Regression is a popular machine learning technique used for both regression and classification tasks. It is an ensemble learning method that combines the predictions of multiple weak learners (often decision trees) to create a strong predictive model. In this context, \"gradient\" refers to the optimization method used to minimize the loss function and improve the model's performance.\n",
    "\n",
    "Here's how Gradient Boosting Regression works:\n",
    "\n",
    "1. Base Learners: Gradient Boosting starts with an initial weak learner, often a simple decision tree (called a \"stump\"), which makes predictions based on a single feature or a small subset of features.\n",
    "\n",
    "2. Residual Learning: It then sequentially adds more decision trees to the ensemble. Each new tree is trained to correct the errors made by the previously added trees. Instead of fitting the target values directly, the new tree is trained on the residuals (the differences between the actual target values and the predictions made by the current ensemble).\n",
    "\n",
    "3. Gradient Descent: The algorithm uses gradient descent optimization to find the best parameters (weights) for each new tree, aiming to minimize the loss function (usually mean squared error for regression problems). Gradient descent is an iterative optimization method that gradually adjusts the parameters in the direction of the steepest descent of the loss function.\n",
    "\n",
    "4. Learning Rate: To control the contribution of each tree to the ensemble, a learning rate is introduced. The learning rate scales the predictions made by each tree before adding them to the overall ensemble. A lower learning rate can improve the generalization of the model but may require more trees to achieve high accuracy.\n",
    "\n",
    "5. Ensemble Prediction: The final prediction is obtained by combining the predictions of all the individual trees, often with the learning rate applied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7d65e2",
   "metadata": {},
   "source": [
    "## 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ecf4f4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e2bcc94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 2)\n",
    "y = 4 * X[:, 0] + 3 * X[:, 1] + 2 * np.random.randn(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d45c1ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "36fc8aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "GBC = GradientBoostingRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "00028eed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor()"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GBC.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "62438971",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = GBC.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "00a1b8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE = mean_squared_error(y_test,y_pred)\n",
    "R2Score = r2_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bc65bbef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5603611719250954\n",
      "0.3436085306451624\n"
     ]
    }
   ],
   "source": [
    "print(MSE)\n",
    "print(R2Score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec920ddc",
   "metadata": {},
   "source": [
    "## 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8edbdfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "422af130",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter = {\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [1, 2, 3],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "98670432",
   "metadata": {},
   "outputs": [],
   "source": [
    "GBC1 = GradientBoostingRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "da773e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "GSC = GridSearchCV(GBC1, parameter, cv = 5, scoring='r2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fca4c43b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=GradientBoostingRegressor(),\n",
       "             param_grid={'learning_rate': [0.01, 0.1, 0.2],\n",
       "                         'max_depth': [1, 2, 3],\n",
       "                         'n_estimators': [100, 200, 300]},\n",
       "             scoring='r2')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GSC.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "341dab43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 200}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GSC.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f309fa57",
   "metadata": {},
   "outputs": [],
   "source": [
    "GBR = GradientBoostingRegressor(learning_rate = 0.01, max_depth = 1, n_estimators = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c03b0467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(learning_rate=0.01, max_depth=1, n_estimators=200)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GBR.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ecf84d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred1 = GBR.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3ac67f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE1 = mean_squared_error(y_test,y_pred1)\n",
    "R2Score1 = r2_score(y_test,y_pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cda798ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6087014123195202\n",
      "0.3346964793421464\n"
     ]
    }
   ],
   "source": [
    "print(MSE1)\n",
    "print(R2Score1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fd969a",
   "metadata": {},
   "source": [
    "## 4.\n",
    "\n",
    "In Gradient Boosting, a weak learner, also known as a base learner or a weak classifier/regressor, refers to a simple model that performs slightly better than random guessing on a given learning task. These models are typically simple and have modest predictive power compared to more complex models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd226f9",
   "metadata": {},
   "source": [
    "## 5.\n",
    "\n",
    "The intuition behind the Gradient Boosting algorithm can be best understood by breaking it down into its two key components: \"Gradient\" and \"Boosting.\"\n",
    "\n",
    "1. Boosting: Boosting is an ensemble learning technique that combines multiple weak learners (often decision trees) to create a strong learner. A weak learner is a model that performs slightly better than random chance, but it doesn't have high predictive power on its own. Boosting sequentially builds a series of weak learners, each focusing on the mistakes made by its predecessors. The final prediction is a weighted sum of the predictions from all weak learners, with more weight given to the more accurate models.\n",
    "\n",
    "2. Gradient: The \"gradient\" in Gradient Boosting refers to the gradient of a loss function with respect to the predictions made by the ensemble. In simpler terms, it's a measure of how much the loss function will change if we make slight adjustments to the model's predictions. By optimizing this loss function, we aim to improve the model's performance in each iteration.\n",
    "\n",
    "Intuition Step by Step:\n",
    "\n",
    "1. Initialization: The boosting process starts by creating the first weak learner (often a decision tree). The initial predictions of this model are quite poor because it's a weak learner.\n",
    "\n",
    "2. Residuals: To improve the predictions, we calculate the residuals, which are the differences between the true values and the predictions made by the current model. These residuals represent the errors that need to be corrected.\n",
    "\n",
    "3. Fit the Next Weak Learner: The next weak learner is trained to predict the residuals of the previous model. It focuses on the mistakes made by the previous model and tries to correct them.\n",
    "\n",
    "4. Weighted Combination: The predictions of the new weak learner are combined with the predictions from all previous models, each multiplied by a weight representing its contribution to the ensemble.\n",
    "\n",
    "5. Update Predictions: The updated predictions are again compared to the true values, and new residuals are calculated. The process of adding weak learners, predicting residuals, and updating the ensemble continues for a specified number of iterations (controlled by the number of trees or the learning rate).\n",
    "\n",
    "6. Final Prediction: The final prediction is obtained by combining the predictions from all weak learners, giving more weight to those with better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc82af9b",
   "metadata": {},
   "source": [
    "## 6.\n",
    "\n",
    "Gradient boosting is an ensemble learning algorithm that builds an ensemble of weak learners, typically decision trees, to make predictions. The algorithm works by iteratively adding new weak learners to the ensemble, each of which is trained to correct the errors of the previous learners.\n",
    "\n",
    "The gradient boosting algorithm works as follows:\n",
    "\n",
    "1. The algorithm starts with a baseline prediction, such as the mean of the target variable.\n",
    "2. A weak learner is trained to minimize the residuals between the baseline prediction and the actual target values.\n",
    "3. The residuals are then updated to reflect the errors made by the weak learner.\n",
    "4. A new weak learner is trained to minimize the residuals again.\n",
    "5. Steps 3 and 4 are repeated until the desired number of weak learners has been added to the ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382168b6",
   "metadata": {},
   "source": [
    "## 7.\n",
    "\n",
    "The mathematical intuition of Gradient Boosting algorithm can be constructed in the following steps:\n",
    "\n",
    "1. Choose a loss function. The loss function is a measure of how well the model is performing. The most common loss functions for gradient boosting are the mean squared error (MSE) and the binary cross-entropy.\n",
    "\n",
    "2. Initialize the model. The model is initialized with some initial predictions. These predictions can be made using a simple model, such as a constant or a linear regression model.\n",
    "\n",
    "3. Calculate the gradients. The gradients are the derivatives of the loss function with respect to the model predictions. The gradients indicate the direction in which the model predictions need to be updated in order to reduce the loss.\n",
    "\n",
    "4. Update the model predictions. The model predictions are updated in the direction of the gradients. The amount by which the predictions are updated is determined by the learning rate.\n",
    "\n",
    "5. Repeat steps 3-4 until the loss function is minimized. The process of updating the model predictions is repeated until the loss function is minimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767381f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cbc237",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d564fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f7c12c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
