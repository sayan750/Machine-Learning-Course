{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "626eaa0a",
   "metadata": {},
   "source": [
    "### 1.\n",
    "\n",
    "R-squared is a statistical measure used to evaluate the goodness of fit of a linear regression model. It indicates the proportion of the variance in the dependent variable that is explained by the independent variables included in the model.\n",
    "\n",
    "R-squared is calculated as the ratio of the explained sum of squares (ESS) to the total sum of squares (TSS). ESS represents the sum of squared differences between the predicted values by the model and the mean of the dependent variable. TSS represents the sum of squared differences between the actual values of the dependent variable and its mean. Mathematically, the formula for R-squared is:\n",
    "\n",
    "R-squared = ESS / TSS\n",
    "\n",
    "R-squared ranges from 0 to 1. A value of 0 indicates that the model explains none of the variability in the dependent variable, while a value of 1 indicates that the model explains all the variability. In general, higher R-squared values indicate a better fit of the model to the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903a68d0",
   "metadata": {},
   "source": [
    "### 2.\n",
    "\n",
    "Adjusted R-squared is a statistical measure used to assess the goodness-of-fit of a regression model, similar to regular R-squared. However, adjusted R-squared incorporates the number of predictors or independent variables in the model, providing a more accurate evaluation of the model's performance.\n",
    "\n",
    "Regular R-squared measures the proportion of the total variation in the dependent variable that can be explained by the independent variables. It ranges from 0 to 1, with 1 indicating a perfect fit. However, regular R-squared tends to increase with the addition of more predictors, regardless of their actual contribution to the model's predictive power.\n",
    "\n",
    "On the other hand, adjusted R-squared penalizes the addition of unnecessary predictors by considering the number of predictors and the sample size. It adjusts for the degrees of freedom in the model, reflecting the trade-off between model complexity and fit. Therefore, adjusted R-squared provides a more conservative estimate of the model's explanatory power, helping to avoid overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62e3bfe",
   "metadata": {},
   "source": [
    "### 3.\n",
    "\n",
    "Adjusted R-squared is more appropriate to use when comparing models with different numbers of predictors or when dealing with overfitting issues. Unlike regular R-squared, which tends to increase with the addition of predictors even if they are not meaningful, adjusted R-squared penalizes the inclusion of irrelevant variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b0e460",
   "metadata": {},
   "source": [
    "### 4.\n",
    "\n",
    "RMSE, MSE, and MAE are commonly used metrics in regression analysis to evaluate the performance of regression models. Here's a brief explanation of each metric:\n",
    "\n",
    "1. Root Mean Squared Error (RMSE): RMSE is the square root of the mean of the squared differences between the predicted values and the actual values. It measures the average magnitude of the errors between predicted and observed values in the same unit as the target variable. RMSE is sensitive to outliers and larger errors due to the squaring operation.\n",
    "\n",
    "2. Mean Squared Error (MSE): MSE is the mean of the squared differences between the predicted values and the actual values. Similar to RMSE, it quantifies the average squared error between the predicted and observed values. Since it involves squaring the errors, MSE emphasizes larger errors more than MAE.\n",
    "\n",
    "3. Mean Absolute Error (MAE): MAE is the mean of the absolute differences between the predicted values and the actual values. It measures the average magnitude of the errors without considering their direction. MAE is less sensitive to outliers compared to RMSE and MSE because it does not involve squaring the errors.\n",
    "\n",
    "To calculate these metrics, you need a set of predicted values (天) and corresponding actual values (y). For each metric:\n",
    "\n",
    "- RMSE: Calculate the squared difference between 天 and y for each observation, take the average of these squared differences, and then take the square root of the average.\n",
    "- MSE: Calculate the squared difference between 天 and y for each observation, and then take the average of these squared differences.\n",
    "- MAE: Calculate the absolute difference between 天 and y for each observation, and then take the average of these absolute differences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc82ac87",
   "metadata": {},
   "source": [
    "### 5.\n",
    "\n",
    "RMSE (Root Mean Squared Error), MSE (Mean Squared Error), and MAE (Mean Absolute Error) are commonly used evaluation metrics in regression analysis. Each metric has its advantages and disadvantages:\n",
    "\n",
    "**Advantages of RMSE:-**\n",
    "\n",
    "1. RMSE penalizes larger errors more than MSE and MAE, making it sensitive to outliers. This can be advantageous when outliers have a significant impact on the analysis.\n",
    "2. RMSE is a well-behaved mathematical property, as it is the square root of the MSE. It has a clear interpretation in the same units as the target variable.\n",
    "3. RMSE is differentiable, which means it can be used in optimization algorithms for model training.\n",
    "\n",
    "Disadvantages of RMSE:\n",
    "\n",
    "1. RMSE is sensitive to the scale of the target variable. If the scale varies across different datasets or experiments, it becomes difficult to compare RMSE values directly.\n",
    "2. RMSE gives more weight to larger errors, which may not always be desired. In certain cases, smaller errors might be more important and should be given higher weight.\n",
    "\n",
    "**Advantages of MSE:-**\n",
    "\n",
    "1. MSE, like RMSE, penalizes larger errors more than MAE, which can be useful when outliers need to be emphasized.\n",
    "2. MSE is also a differentiable and well-behaved mathematical property, making it suitable for optimization algorithms.\n",
    "\n",
    "Disadvantages of MSE:\n",
    "\n",
    "1. Similar to RMSE, MSE is influenced by the scale of the target variable, making direct comparisons challenging.\n",
    "2. The squared nature of MSE magnifies the impact of outliers, which can lead to misleading evaluations of the model's performance.\n",
    "3. The interpretation of MSE might not be intuitive as it is in squared units compared to the original target variable.\n",
    "\n",
    "**Advantages of MAE:-**\n",
    "\n",
    "1. MAE is less sensitive to outliers than RMSE and MSE, making it a robust metric for situations where outliers are present.\n",
    "2. MAE provides a more intuitive interpretation as it is in the same units as the target variable, allowing for easier communication of the model's performance.\n",
    "3. MAE treats all errors equally, which can be desirable when all errors are considered equally important.\n",
    "\n",
    "Disadvantages of MAE:\n",
    "\n",
    "1. MAE does not differentiate between small and large errors, which may not be suitable for scenarios where larger errors should be emphasized.\n",
    "2. MAE is not differentiable at zero, which may limit its use in certain optimization algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d502c128",
   "metadata": {},
   "source": [
    "### 6.\n",
    "\n",
    "Lasso regularization, also known as L1 regularization, is a technique used in machine learning to prevent overfitting and improve model performance. It achieves this by adding a penalty term to the model's objective function, which encourages the coefficients of less important features to become zero. This results in feature selection, effectively reducing the number of features used in the model.\n",
    "\n",
    "The key difference between Lasso regularization and Ridge regularization (L2 regularization) lies in the penalty term. While Lasso uses the absolute values of the coefficients (L1 norm) as the penalty, Ridge regularization uses the squared values of the coefficients (L2 norm). This difference leads to distinct behavior in terms of feature selection.\n",
    "\n",
    "Lasso regularization tends to drive the coefficients of less important features to exactly zero, effectively performing feature selection. This makes Lasso regularization useful when dealing with datasets that contain a large number of features, many of which may not be relevant to the target variable. It can help identify the most important features and simplify the model.\n",
    "\n",
    "On the other hand, Ridge regularization tends to shrink the coefficients of less important features towards zero but does not eliminate them entirely. This makes Ridge regularization suitable when all features are potentially relevant and there is no strong prior belief that some should be completely excluded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322a3e33",
   "metadata": {},
   "source": [
    "### 7.\n",
    "\n",
    "Regularized linear models help prevent overfitting in machine learning by introducing a regularization term to the loss function, which penalizes large coefficient values. This regularization encourages the model to find a balance between fitting the training data well and keeping the model parameters small.\n",
    "\n",
    "One common type of regularized linear model is Ridge Regression, which adds the L2 regularization term to the loss function. The L2 regularization term is proportional to the sum of the squared coefficients. By minimizing this term, Ridge Regression encourages the model to distribute the importance among all features rather than relying heavily on a few.\n",
    "\n",
    "Let's consider an example where we want to predict housing prices based on features such as square footage, number of bedrooms, and location. Without regularization, a linear model might assign large coefficients to certain features, such as square footage, to fit the training data perfectly. This can lead to overfitting, where the model performs poorly on new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e16c129",
   "metadata": {},
   "source": [
    "### 8.\n",
    "\n",
    "Regularized linear models, such as ridge regression and lasso regression, have certain limitations that make them not always the best choice for regression analysis. Here are some key limitations:\n",
    "\n",
    "1. Linearity assumption: Linear models assume a linear relationship between the predictors and the response variable. However, in real-world scenarios, the relationship between variables can be more complex and nonlinear. In such cases, linear models may not capture the underlying patterns effectively, leading to suboptimal performance.\n",
    "\n",
    "2. Feature selection: Although regularized linear models can help with feature selection by shrinking the coefficients of irrelevant or less important features, they do not perform explicit feature selection. As a result, if there are a large number of features, these models may not effectively identify and exclude truly irrelevant predictors, leading to overfitting or reduced interpretability.\n",
    "\n",
    "3. Model complexity: Regularized linear models provide a trade-off between bias and variance by introducing a penalty term that shrinks the coefficients. However, in situations where the relationship between predictors and the response is highly complex, linear models may struggle to capture the intricate interactions and non-linear effects. This can result in a high bias, leading to underfitting and limited predictive performance.\n",
    "\n",
    "4. Assumption of homoscedasticity: Linear models assume homoscedasticity, which means the variability of the errors is constant across all levels of the predictors. However, in real-world scenarios, the variability of the errors may not be constant, and this assumption may be violated. In such cases, linear models may provide biased and inefficient estimates.\n",
    "\n",
    "5. Outliers and influential points: Regularized linear models can be sensitive to outliers and influential data points. Outliers can disproportionately affect the estimated coefficients and potentially distort the model's performance. While ridge regression is more robust to outliers compared to lasso regression, extreme influential points can still influence the results.\n",
    "\n",
    "In summary, regularized linear models have limitations in capturing complex nonlinear relationships, performing feature selection, accommodating violation of assumptions, and handling outliers. In cases where the relationship between predictors and the response is highly nonlinear or the dataset contains many irrelevant features, alternative modeling techniques such as decision trees, support vector machines, or neural networks may be more suitable for regression analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a26b70",
   "metadata": {},
   "source": [
    "### 9.\n",
    "\n",
    "Based solely on the provided evaluation metrics, Model B with an MAE of 8 would be considered the better performer compared to Model A with an RMSE of 10.\n",
    "\n",
    "The MAE (Mean Absolute Error) measures the average absolute difference between predicted and actual values, while the RMSE (Root Mean Square Error) measures the square root of the average squared difference between predicted and actual values. In both cases, lower values indicate better performance.\n",
    "\n",
    "However, it is essential to consider the limitations of each metric. The MAE gives equal weight to all errors, while the RMSE penalizes larger errors more heavily due to the squared term. Thus, the choice between MAE and RMSE depends on the specific context and the relative importance of larger errors. RMSE is more sensitive to outliers, whereas MAE is more robust. Additionally, the choice may also depend on domain-specific considerations and the specific goals of the regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce202af",
   "metadata": {},
   "source": [
    "### 10.\n",
    "\n",
    "Based on the given information, it is difficult to determine which model is the better performer without additional context or evaluation metrics. However, I can provide some insights on the trade-offs and limitations of each regularization method.\n",
    "\n",
    "Ridge regularization (L2 regularization) adds the sum of squared coefficients multiplied by a regularization parameter to the loss function. It encourages small but non-zero coefficients, helping to reduce overfitting and handle multicollinearity. Ridge regularization can be effective when there are many features that contribute to the target variable.\n",
    "\n",
    "Lasso regularization (L1 regularization) adds the sum of absolute coefficients multiplied by a regularization parameter to the loss function. It encourages sparse solutions by shrinking some coefficients to zero, effectively performing feature selection. Lasso regularization is beneficial when there are many features but only a few are relevant.\n",
    "\n",
    "The choice between Ridge and Lasso regularization depends on the specific problem and data. Ridge regularization tends to work well when there is a need for all features to contribute to the model's prediction, while Lasso regularization is suitable for feature selection and when there is a desire to simplify the model by focusing on a subset of important features.\n",
    "\n",
    "However, it's important to note that both regularization methods have limitations. Ridge regularization doesn't perform explicit feature selection and can still keep less important features with small coefficients. Lasso regularization can sometimes be sensitive to correlated features and may arbitrarily select one feature over another. Additionally, Lasso regularization may not work well if the number of features is significantly larger than the number of observations.\n",
    "\n",
    "To make a more informed decision about the better performer, you would need to consider additional factors such as the specific data characteristics, evaluation metrics (e.g., accuracy, mean squared error), and potentially perform cross-validation or other model selection techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ee665e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c208f8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead93cd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
